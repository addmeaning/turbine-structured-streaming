{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark structured streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turbine Analytics, 15.02.2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W nawizaniu do poprzedniego głosowania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![polly](https://i.imgur.com/7XL7BBG.png)\n",
    "\n",
    "Dziś pogadamy trochę o Spark structured streaming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPARK (STRUCTURED) STREAMING\n",
    "### Trochę teorii o przetwarzaniu strumieni w Sparku\n",
    "\n",
    "trochę o samym streaming'u\n",
    "\n",
    "##### Batch vs Streaming\n",
    "\n",
    "Przy przekształcaniu batch'owym dla nas dostępny \"komplet\" danych do obróbki.\n",
    "\n",
    "Głowna idea w przekształceniu strumieniowym w tym żeby traktować dane jak tabełę do którę są dodawane ciągle są dodawane nowe linii.\n",
    "\n",
    "\n",
    "Jakie są różnicy pomiędzy tymi trybami przekształeń danych:\n",
    "\n",
    "**Stream**\n",
    "\n",
    "1. Robi przekształcenie albo na jednym elemncie albo na kilku elementach (na oknie) który sytem dostał\n",
    "2. Przekształcenia zazwyczaj proste (ze wzgłędu na potrzebe szybkiej odpowiedzi)\n",
    "3. Obliczenia zazyczaj niezależne (w kontekscie okna lub rekordu)\n",
    "4. Asynchroniczne zazwyczaj źródło danych nie interesuje się u nas czy otrzymaliśmy dane :)\n",
    "\n",
    "**Batch**\n",
    "\n",
    "1. Ma dostęp do wszystkich danych\n",
    "2. Może sobie pozwolić na przekształcenia o dowolnym skomplikowaniu, Bo:\n",
    "3. Job może trwać minuty (i godziny ;)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zalety przetwarzania strumieniowego\n",
    "\n",
    "* Prostsze do zrozumienia — przetwarzania strumieniowe łatwiej pisać i łatwiej debugować\n",
    "* near real time — przetwarzania strumieniowe pozwalają odpowiadać szybko -> sekundy lub milisekundy\n",
    "* latwiejszy update danych — z natury obliczeń inkrementalnych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wyzwania przetwarzania strumieniowego\n",
    "\n",
    "* processowanie wiadomości które przyszli nie po kolei\n",
    "* może być potrzeba trzeba przechowywać dużo informacji o stanie przetwarzania\n",
    "* zapewnienie przetwarzania **1** raz nawet jeżeli coś padnie\n",
    "* zapewnienie nizkiego czasu odpowiedzi\n",
    "* połączenie danych z zewnętrznymi źródłami danych\n",
    "* updejtowanie biznes logiki real time\n",
    "* tranzakcyjny zapis do zewnętrz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dwa różnych API dla Streaming'u \n",
    "\n",
    "Spark ma bogatą historię wsparcia przetwarzania strumieniowego\n",
    "\n",
    "W 2012 do Spark'u dołączył projekt SPark Streaming ze Swoim **DStreams** API.\n",
    "To było jedno z pierwszych API (przynajmniej tak wyszło z mojego researchu :) którę dawało developerom możliwość \n",
    "użycia funkcji wysokopoziomowych, jakich jak _map_ i _reduce_.\n",
    "\n",
    "Dotychczas jest Spark Streaming jest używany w wielu organizacji (patrząc na ilość pytań na StackOverflow).\n",
    "\n",
    "Jednak _DStreams_ jak i RDD opierają się na obiektach Java/Python co w praktyce zawęża pole możliwych optymalizacji).\n",
    "\n",
    "\n",
    "W 2016 więc powstał project Structured Streaming który używa DataFrame'y i Datasety. Structured Streaming API jest oznaczone jako stabilne w wersji Spark'a 2.2.0 więc jest gotowy do komercyjnego użycia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To oznacza że on jest oparty na projekt Spark SQL (który pokazywałem na poprzednim spotkaniu). A to oznacza że **można używać ten sam kod dla batchowych przekształcen jak i dla strumieniowych**\n",
    "\n",
    "Structured streaming pozwala na:\n",
    "* **agregacje** na strumieniach\n",
    "* **uzycia okien czasowych**\n",
    "* joinować strumienie batchowe i strumieniowe.\n",
    "\n",
    "Wszystkie optymalizację, o których mówiłem na poprzednim spotkaniu, czyli query analyzer, cost optimizer\n",
    "\n",
    "![query analyzer](https://i.imgur.com/j6NYnac.png)\n",
    "\n",
    "![cost optimizer](https://i.imgur.com/nja8qJo.png)\n",
    "\n",
    "oraz kastomowy serializator, off heap data storage i td. cą dostępne w structured streaming.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przetwarzanie ciągłę vs Micro-batch przetwarzanie\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**przetwarzenie ciągłe**\n",
    "![one record at a time](https://i.imgur.com/pkQJouq.png)\n",
    "\n",
    "* pozwala na najmnieszy czas odpowiedzi. Natomiast takie systemy mają miejszy throughput, dlatego że mają większy overhead (wysyłanie każdej wiadomości to wywołąnie funkcji OS i która robi pakiet sieciowy i td)\n",
    "* fixed topology. Czyli trudno zmienić tor przekształcenie w runtime.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**przetwarzenie micro-batch**\n",
    "Akumuluje wiadomości w małych batchach (na przykład po 300 ms) i procesuje ten batch job w Spark'u\n",
    "\n",
    "![micro-batch](https://i.imgur.com/b5jGg7B.png)\n",
    "\n",
    "* większa wydajność systemu\n",
    "* pozwazala używać tych optymalizacji o których mówiłem wcześniej.\n",
    "* potrzebują mniej węzłów żeby przeształcić ten sam wolumen danych.\n",
    "\n",
    "\n",
    "Co wybrać:\n",
    "* Jeżeli 100 ms to nie akceptowalne opóźnienie to lepiej wybrać przetwarzanie ciągłe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stare API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val conf = new SparkConf().setAppName(\"SparkStreamingExampleApp\").setMaster(\"local[*]\")\n",
    "val ssc = new StreamingContext(conf, Seconds(5))\n",
    "\n",
    "///ssc.textFileStream(\"path\")\n",
    "///ssc.queueStream(queue: mutable.Queue[RDD[T]], oneAtATime: Boolean = true): InputDStream[T]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nowe API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "//batch df\n",
    "val df: DataFrame = spark.read.csv(\"sales\")//options\n",
    "df.isStreaming //return false\n",
    "\n",
    "val streamingDf:DataFrame = spark.readStream.format(\"rate\").load\n",
    "streamingDf.isStreaming//returns true\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
